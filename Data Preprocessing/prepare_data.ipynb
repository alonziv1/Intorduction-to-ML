{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alonziv1/Machine-Learning/blob/main/prepare_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIzoJJV2w5Hb"
      },
      "source": [
        "**Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgUWGh0Ww_VZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from numpy import nan\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn as sk\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJTwEV9vtTle"
      },
      "source": [
        "#**Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IB7eThoq14q"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data, training_data):\n",
        "\n",
        "    raw_data = data.copy()\n",
        "    raw_training_data = training_data.copy()\n",
        "\n",
        "    raw_data.reset_index(inplace = True)\n",
        "    raw_training_data.reset_index(inplace = True)\n",
        "\n",
        "    raw_data = select_features(raw_data)\n",
        "    raw_training_data = select_features(raw_training_data)\n",
        "\n",
        "    prepared_data = transform_features(raw_data)\n",
        "    prepared_training_data = transform_features(raw_training_data)\n",
        "   \n",
        "    prepared_data, prepared_training_data = mean_imputate_features(prepared_data, prepared_training_data)\n",
        "    prepared_data, prepared_training_data = median_imputate_features(prepared_data, prepared_training_data)\n",
        "    prepared_data, prepared_training_data = most_freq_imputate_features(prepared_data, prepared_training_data)\n",
        "\n",
        "    prepared_training_data = select_features_after(prepared_training_data)\n",
        "    prepared_data = select_features_after(prepared_data)\n",
        "\n",
        "    prepared_data = normalize_features(prepared_data, prepared_training_data)\n",
        "\n",
        "    prepared_data = prepared_data[sorted(prepared_data.columns)]\n",
        "\n",
        "    return prepared_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77AJAh6KteNz"
      },
      "source": [
        "#**select_features**\n",
        "\n",
        "according to our analysis we chose a subgroup of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsKgIgu8UxlQ"
      },
      "outputs": [],
      "source": [
        "def select_features(data):\n",
        "  _data = data[['PCR_01','PCR_07','PCR_04','PCR_08','PCR_10', 'PCR_05','sport_activity','sugar_levels', 'symptoms','blood_type','sex','covid','spread','risk']]\n",
        "  return _data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlIDG3-QE5JS"
      },
      "outputs": [],
      "source": [
        "def select_features_after(data):\n",
        "  return data.drop(columns = ['low_appetite', 'sex', 'A-', 'AB+', 'B+', 'B-', 'O+', 'O-'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObV8sPKMt7ba"
      },
      "source": [
        "#**transform_features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YevP2RtouLq4"
      },
      "outputs": [],
      "source": [
        "def transform_features(data):\n",
        "\n",
        "  string_to_numeric(data)\n",
        "  data = one_hot_encoding(data)\n",
        "  unique_symptoms = get_symptoms(data)\n",
        "  data = add_symptoms_features(data, unique_symptoms)\n",
        "  string_to_numeric(data)\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOaKfYy5v-7I"
      },
      "source": [
        "##**string_to_numeric**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIarS6eWwHqx"
      },
      "outputs": [],
      "source": [
        "def string_to_numeric(data):\n",
        "  data.replace({\"High\": 1, \"Low\": 0}, inplace=True)\n",
        "  data.replace({\"F\": 1, \"M\": 0}, inplace=True)\n",
        "  data.replace({True: 1, False: 0}, inplace=True)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dBhCWrOwp85"
      },
      "source": [
        "##**one_hot_encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eg8q8nnwsqa"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoding(data):\n",
        "  \n",
        "  blood_type_num = pd.get_dummies(data['blood_type'])\n",
        "  joined_data = data.join(blood_type_num)\n",
        "  joined_data.drop(['blood_type'], axis = 1, inplace = True)\n",
        "\n",
        "  return joined_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVfoVPMlybtx"
      },
      "source": [
        "##**get_symptoms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUrnNhDFyfYy"
      },
      "outputs": [],
      "source": [
        "def get_symptoms(data):\n",
        "  symptoms_list = data['symptoms'].unique()\n",
        "  unique_symptoms = []\n",
        "  for i in symptoms_list:\n",
        "    if (type(i) is str):\n",
        "      unique_symptoms.append(i.split(\";\")) \n",
        "\n",
        "  unique_symptoms = list(itertools.chain.from_iterable(unique_symptoms))\n",
        "  unique_symptoms = pd.Series(unique_symptoms)\n",
        "  unique_symptoms = unique_symptoms.unique()\n",
        "\n",
        "  return unique_symptoms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psNSn_UJzitJ"
      },
      "source": [
        "##**add_symptoms_features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-v1U3LDzk_h"
      },
      "outputs": [],
      "source": [
        "def add_symptoms_features(data, unique_symptoms):\n",
        "  \n",
        "  symptoms_df = pd.DataFrame(index=range(data.shape[0]))\n",
        "  for symptom in unique_symptoms:\n",
        "    symptoms_df[symptom] = np.nan\n",
        "  symptoms_df.fillna(0, inplace=True)\n",
        "  symptoms_df[np.isnan(symptoms_df)] = 0\n",
        "\n",
        "  \n",
        "  joined_data = data.join(symptoms_df)\n",
        "\n",
        "  for index in joined_data.index:\n",
        "    if(type(joined_data['symptoms'][index]) is not str):\n",
        "      continue\n",
        "    for symptom in unique_symptoms:\n",
        "      if (symptom in joined_data['symptoms'][index]):\n",
        "        joined_data[symptom][index] = 1\n",
        "\n",
        "  joined_data.drop(['symptoms'], axis = 1, inplace = True)\n",
        "  \n",
        "  return joined_data     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoqFHlMq7rXB"
      },
      "source": [
        "##**imputate_features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C1gkzlO734x"
      },
      "outputs": [],
      "source": [
        "def mean_imputate_features(data, training_data):\n",
        "  \n",
        "    mean_features = ['PCR_01','PCR_07','PCR_04','PCR_05', 'PCR_08', 'PCR_10', 'sugar_levels','sport_activity','shortness_of_breath','sore_throat']\n",
        "    imputer1 = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    training_data[mean_features] = imputer1.fit_transform(training_data[mean_features])\n",
        "    data[mean_features] = imputer1.transform(data[mean_features])\n",
        "\n",
        "    return data, training_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def median_imputate_features(data, training_data):\n",
        "  \n",
        "    features = ['sport_activity']\n",
        "    imputer1 = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "    training_data[features] = imputer1.fit_transform(training_data[features])\n",
        "    data[features] = imputer1.transform(data[features])\n",
        "\n",
        "    return data, training_data"
      ],
      "metadata": {
        "id": "QmuNPE07cmcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOraWK9oY1h3"
      },
      "outputs": [],
      "source": [
        "def most_freq_imputate_features(data, training_data):\n",
        "\n",
        "    imputer2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "    training_data[['sex', 'A+']] = imputer2.fit_transform(training_data[['sex', 'A+']])\n",
        "    data[['sex', 'A+']] = imputer2.transform(data[['sex', 'A+']])\n",
        "\n",
        "    return data, training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LolG9qai7uDf"
      },
      "source": [
        "##**normalize_features**\n",
        "\n",
        "we are using min - max scaling, since it performed better then standard scaling for all features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRdkfAEA7xcw"
      },
      "outputs": [],
      "source": [
        "def normalize_features(data, training_data):\n",
        "  \n",
        "  from sklearn import preprocessing\n",
        "\n",
        "  scaler = preprocessing.MinMaxScaler().fit(training_data)\n",
        "\n",
        "  scaled_data = scaler.transform(data)\n",
        "\n",
        "  data.loc[:,:] = scaled_data\n",
        "\n",
        "  return data\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "prepare data",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}